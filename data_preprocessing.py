# -*- coding: utf-8 -*-
"""data_preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14bIhXFi1njK6_w0iCejtVbIRMoiS3VmZ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import os
import time
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import json
import itertools
import sys

os.chdir('/content/')
import gdown, os
gdown.download("https://drive.google.com/uc?id=1YWvvHLuS7dGa4wsdOGUI-c3r3qsH9Cmn", 
               quiet=False)
!unzip -n ta_feng_all_months_merged.csv.zip
ta_feng = pd.read_csv('/content/ta_feng_all_months_merged.csv')
os.chdir('/content/recanet/')

ta_feng.TRANSACTION_DT = pd.to_datetime(ta_feng.TRANSACTION_DT)

ta_feng_2 = ta_feng.groupby(['TRANSACTION_DT', 'CUSTOMER_ID'])['PRODUCT_ID'].apply(list).reset_index()  
baskets_unique = [list(x) for x in set(tuple(x) for x in ta_feng_2.PRODUCT_ID.values)] 

#basket_id_mapper = {}
id_basket_mapper = {}
for i in range(len(baskets_unique)):
    #basket_id_mapper[baskets_unique[i]] = i+1
    id_basket_mapper[i+1] = baskets_unique[i] 

inv_mapper = {tuple(value): key for key, value in id_basket_mapper.items()}
ta_feng_2['basket_id'] = ta_feng_2.PRODUCT_ID.apply(lambda x: inv_mapper[tuple(x)])
ta_feng_baskets = ta_feng.merge(ta_feng_2, on=['TRANSACTION_DT', 'CUSTOMER_ID'])

ta_feng_baskets.rename(columns={'PRODUCT_ID_x': 'item_id', 'PRODUCT_ID_y': 'BASKET',
                                'CUSTOMER_ID': 'user_id', 'TRANSACTION_DT': 'date'}, inplace=True)

import pandas as pd
import numpy as np

'''
Reads the raw files, renames columns, last basket as test and the rest as train.
No additional preprocessing steps.
'''

input_file_path = 'dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/transaction_data.csv'
train_baskets_file_path = 'data/dunnhumby_cj/train_baskets.csv'
test_baskets_file_path = 'data/dunnhumby_cj/test_baskets.csv'
valid_baskets_file_path = 'data/dunnhumby_cj/valid_baskets.csv'

df = pd.read_csv(input_file_path)
print(df.shape)
df['date'] = df['DAY'].astype(int)
df['basket_id'] = df['BASKET_ID']
df['item_id'] = df['PRODUCT_ID'].astype(str)
df['user_id'] = df['household_key'].astype(str)

processed_df = df[['date','basket_id','user_id','item_id']].drop_duplicates()
print(processed_df.shape)
print(processed_df.nunique())
last_baskets = processed_df[['user_id','basket_id','date']].drop_duplicates() \
    .groupby('user_id').apply(lambda grp: grp.nlargest(1, 'date'))
last_baskets.index = last_baskets.index.droplevel()
test_baskets = pd.merge(last_baskets, processed_df, how='left')


train_baskets = pd.concat([processed_df,test_baskets]).drop_duplicates(keep=False)

all_users = list(set(test_baskets['user_id'].tolist()))
valid_indices = np.random.choice(range(len(all_users)),int(0.5*len(all_users)),
                                 replace=False)
valid_users = [all_users[i] for i in valid_indices]

valid_baskets = test_baskets[test_baskets['user_id'].isin(valid_users)]
test_baskets = test_baskets[~test_baskets['user_id'].isin(valid_users)]

print(valid_baskets.shape)
print(test_baskets.shape)
print(train_baskets.shape)

print(valid_baskets.nunique())
print(test_baskets.nunique())
print(train_baskets.nunique())

train_baskets.to_csv(train_baskets_file_path,index=False)
test_baskets.to_csv(test_baskets_file_path,index=False)
valid_baskets.to_csv(valid_baskets_file_path,index=False)




import pandas as pd
import numpy as np

'''
Reads the raw files, renames columns, last basket as test and the rest as train.
No additional preprocessing steps.
'''


prior_orders_file_path = 'instacart_2017_05_01/order_products__prior.csv'
train_orders_file_path = 'instacart_2017_05_01/order_products__train.csv'
orders_file_path = 'instacart_2017_05_01/orders.csv'
train_baskets_file_path = 'data/instacart/train_baskets.csv'
test_baskets_file_path = 'data/instacart/test_baskets.csv'
valid_baskets_file_path = 'data/instacart/valid_baskets.csv'

prior_orders = pd.read_csv(prior_orders_file_path)
train_orders = pd.read_csv(train_orders_file_path)
all_orders = pd.concat([prior_orders,train_orders])
#print(all_orders.shape)
#print(all_orders.nunique())

order_info = pd.read_csv(orders_file_path)

all_orders = pd.merge(order_info,all_orders,how='inner')
print(all_orders.shape)
print(all_orders.nunique())
#print(all_orders.head())

all_orders = all_orders.rename(columns={'order_id':'basket_id', 'product_id':'item_id'})



#all_users = list(set(all_orders['user_id'].tolist()))
#random_users_indices = np.random.choice(range(len(all_users)), 10000, replace=False)
#random_users = [all_users[i] for i in range(len(all_users)) if i in random_users_indices]
#all_orders = all_orders[all_orders['user_id'].isin(random_users)]


last_baskets = all_orders[['user_id','basket_id','order_number']].drop_duplicates() \
    .groupby('user_id').apply(lambda grp: grp.nlargest(1, 'order_number'))
last_baskets.index = last_baskets.index.droplevel()
test_baskets = pd.merge(last_baskets, all_orders, how='left')
train_baskets = pd.concat([all_orders,test_baskets]).drop_duplicates(keep=False)

all_users = list(set(test_baskets['user_id'].tolist()))
valid_indices = np.random.choice(range(len(all_users)),int(0.5*len(all_users)),
                                 replace=False)
valid_users = [all_users[i] for i in valid_indices]

valid_baskets = test_baskets[test_baskets['user_id'].isin(valid_users)]
test_baskets = test_baskets[~test_baskets['user_id'].isin(valid_users)]

print(valid_baskets.shape)
print(test_baskets.shape)
print(train_baskets.shape)

print(valid_baskets.nunique())
print(test_baskets.nunique())
print(train_baskets.nunique())

train_baskets.to_csv(train_baskets_file_path,index=False)
test_baskets.to_csv(test_baskets_file_path,index=False)
valid_baskets.to_csv(valid_baskets_file_path,index=False)



import pandas as pd
import numpy as np

'''
Reads the raw files, renames columns, last basket as test and the rest as train.
No additional preprocessing steps.
'''

#input_file_path = 'dunnhumby_The-Complete-Journey/dunnhumby - The Complete Journey CSV/transaction_data.csv'
train_baskets_file_path = '/content/drive/MyDrive/tafeng/train_baskets.csv'
test_baskets_file_path = '/content/drive/MyDrive/tafeng/test_baskets.csv'
valid_baskets_file_path = '/content/drive/MyDrive/tafeng/valid_baskets.csv'

df = ta_feng_baskets
print(df.shape)
df['date'] = df['date']
#.astype(int)
df['basket_id'] = df['basket_id']
df['item_id'] = df['item_id'].astype(str)
df['user_id'] = df['user_id'].astype(str)

processed_df = df[['date','basket_id','user_id','item_id']].drop_duplicates()
print(processed_df.shape)
print(processed_df.nunique())
last_baskets = processed_df[['user_id','basket_id','date']].drop_duplicates() \
    .groupby('user_id').apply(lambda grp: grp.nlargest(1, 'date'))
last_baskets.index = last_baskets.index.droplevel()
test_baskets = pd.merge(last_baskets, processed_df, how='left')


train_baskets = pd.concat([processed_df,test_baskets]).drop_duplicates(keep=False)

all_users = list(set(test_baskets['user_id'].tolist()))
valid_indices = np.random.choice(range(len(all_users)),int(0.5*len(all_users)),
                                 replace=False)
valid_users = [all_users[i] for i in valid_indices]

valid_baskets = test_baskets[test_baskets['user_id'].isin(valid_users)]
test_baskets = test_baskets[~test_baskets['user_id'].isin(valid_users)]

print(valid_baskets.shape)
print(test_baskets.shape)
print(train_baskets.shape)

print(valid_baskets.nunique())
print(test_baskets.nunique())
print(train_baskets.nunique())

train_baskets.to_csv(train_baskets_file_path,index=False)
test_baskets.to_csv(test_baskets_file_path,index=False)
valid_baskets.to_csv(valid_baskets_file_path,index=False)

