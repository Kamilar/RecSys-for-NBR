# -*- coding: utf-8 -*-
"""baseline_calculating

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FVRD8N6k3EJT9aBDwUl5hfSn1-IELmo-
"""

import torch
torch.cuda.is_available()

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import os
import time
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score
import json
import itertools
import sys
from scipy.sparse import csr_matrix, vstack
from scipy import sparse

basket_count_min = 3
min_item_count = 5
history_len = 20

# preparind dataset for torch.dataset class
class PreDataset():
    def __init__(self, path_train,path_val, path_test, dataset, history_len, basket_count_min=0,\
                                                         min_item_count = 0):
        self.basket_count_min = basket_count_min
        self.min_item_count = min_item_count
        self.history_len = history_len
        
        self.train_baskets = pd.read_csv(path_train)
        self.valid_baskets = pd.read_csv(path_val)
        self.test_baskets = pd.read_csv(path_test)
        
        basket_per_user = self.train_baskets[['user_id','basket_id']].drop_duplicates() \
            .groupby('user_id').agg({'basket_id':'count'}).reset_index()
        self.test_users = basket_per_user[basket_per_user['basket_id'] >= self.basket_count_min]['user_id'].tolist()
    
        print("number of test users:", len(self.test_users))
        
        self.model_name = 'data/dunnhumby_cj/'+dataset+ '_recanet'
        self.dataset = dataset
        self.all_items = self.train_baskets[['item_id']].drop_duplicates()['item_id'].tolist()
        self.all_users = self.train_baskets[['user_id']].drop_duplicates()['user_id'].tolist()
        self.num_items = len(self.all_items) +1
        self.num_users = len(self.all_users) +1

        print("items:", self.num_items)
        item_counts = self.train_baskets.groupby(['item_id']).size().to_frame(name = 'item_count').reset_index()
        item_counts = item_counts[item_counts['item_count']>= min_item_count]
        item_counts_dict = dict(zip(item_counts['item_id'],item_counts['item_count']))
        print("filtered items:", len(item_counts_dict))
        self.num_items = len(item_counts_dict) +1
        self.item_id_mapper = {} 
        self.id_item_mapper = {}
        self.user_id_mapper = {}
        self.id_user_mapper = {}

        counter = 0
        for i in range(len(self.all_items)):
            if self.all_items[i] in item_counts_dict:
                self.item_id_mapper[self.all_items[i]] = counter+1
                self.id_item_mapper[counter+1] = self.all_items[i]
                counter+=1
        for i in range(len(self.all_users)):
            self.user_id_mapper[self.all_users[i]] = i+1
            self.id_user_mapper[i+1] = self.all_users[i]
            
            
    def create_train_data(self):
        #if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(20) + '_train_users.npy'):
        if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(20) + '_train_users.npy'):
            print('Data allready in use')
            train_users = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_users.npy')
            train_items = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_items.npy')
      
            #train_history2 = np.load('Downloads/dunnhumby3/dunnhumby_upd/'+ str(self.history_len) + '_train_history2.npy')
            train_history2 = sparse.load_npz('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_history2.npz')
            train_labels = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len)+ '_train_labels.npy')
            return train_items,train_users, train_history2, train_labels
        row_counts = 0
        basket_items = self.train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()
        basket_items_dict = dict(zip(basket_items['basket_id'],basket_items['item_id']))
        basket_items_dict['null'] = []

        user_baskets = self.train_baskets[['user_id','date','basket_id']].drop_duplicates().\
            sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()

        user_baskets_dict = dict(zip(user_baskets['user_id'],user_baskets['basket_id']))


        train_users = []
        train_items = []
        train_history = []
        train_history2 = []
        train_labels = []
        print('num users:', len(self.test_users))
        
        for c, user in tqdm(enumerate(self.test_users)):
            if c % 1000 ==1:
                print(c , 'user passed')

            baskets = user_baskets_dict[user]
            item_seq = {}
            for i, basket in enumerate(baskets):
                for item in basket_items_dict[basket]:
                    if item not in self.item_id_mapper:
                        continue
                    if item not in item_seq:
                        item_seq[item] = []
                    item_seq[item].append(i)


            for i in range(max(0,len(baskets)-50), len(baskets)):
                label_basket = baskets[i]
                all_history_baskets = baskets[:i]
                items = []
                for basket in all_history_baskets:
                    for item in basket_items_dict[basket]:
                        items.append(item)
                items = list(set(items))
                for item in items:
                    if item not in self.item_id_mapper:
                        continue
                    index = np.argmax(np.array(item_seq[item])>=i)
                    if np.max(np.array(item_seq[item])) < i:
                        index = len(item_seq[item])
                    input_history = item_seq[item][:index].copy()
                    if len(input_history) ==0:
                        continue
                    if len(input_history) ==1 and input_history[0]==-1:
                        continue
                    while len(input_history) < self.history_len:
                        input_history.insert(0,-1)

                    real_input_history2 = []
                    for j,x in enumerate(input_history[:-1]):
                        if x == -1:
                            real_input_history2.append(0)
                        else:
                            real_input_history2.append(input_history[j+1]-input_history[j])
                    real_input_history2.append(i-input_history[-1])
                    train_users.append(self.user_id_mapper[user])
                    train_items.append(self.item_id_mapper[item])
                        
                    #train_history.append(real_input_history[-self.history_len:])
                    train_history2.append(real_input_history2[-self.history_len:])
                    #print(item, basket_items_dict[label_basket])
                    train_labels.append(float(item in basket_items_dict[label_basket]))

                    row_counts +=1
            print(row_counts)

        train_items = np.array(train_items)
        train_users = np.array(train_users)
        #train_history = np.array(train_history)
        train_history2 = np.array(train_history2)
        train_labels = np.array(train_labels)
        random_indices = np.random.choice(range(len(train_items)), len(train_items),replace=False)
        train_items = train_items[random_indices]
        train_users = train_users[random_indices]
        #train_history = train_history[random_indices]
        train_history2 = train_history2[random_indices]
        train_labels = train_labels[random_indices]  
        
        train_history2 = csr_matrix(train_history2)

        #np.save(self.model_name +'_' + str(self.history_len) + '_train_items.npy',train_items)
        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_items.npy', train_items)
        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_users.npy', train_users)
      
       # np.save('Downloads/dunn/'+ str(self.history_len) + '_train_history2.npy', train_history2)
        sparse.save_npz('Downloads/dunn/'+ str(self.history_len) + '_train_history2.npz', train_history2)
        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_labels.npy', train_labels)

        return train_items,train_users, train_history2 , train_labels
    
    
    def create_test_data(self,test_data='test'):
        
        if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(self.history_len)+ '_'+test_data+'_users.npy'):
            test_users = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_users.npy')
            test_items = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_items_30k.npy')
        
            #test_history2 = sparse.load_npz('Downloads/dunnhumby3/'+ str(self.history_len) + '_'+test_data+'_history2.npz')
            test_history2 = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_history2.npy')
            test_labels = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_labels.npy')
            return test_items,test_users,test_history2, test_labels

        train_basket_items = self.train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()
        train_basket_items_dict = dict(zip(train_basket_items['basket_id'],train_basket_items['item_id']))

        train_user_baskets = self.train_baskets[['user_id','date','basket_id']].drop_duplicates(). \
            sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()
        train_user_baskets_dict = dict(zip(train_user_baskets['user_id'],train_user_baskets['basket_id']))

        train_user_items = self.train_baskets[['user_id','item_id']].drop_duplicates().groupby(['user_id'])['item_id'] \
            .apply(list).reset_index()
        train_user_items_dict = dict(zip(train_user_items['user_id'],train_user_items['item_id']))

        test_user_items = None
        if test_data == 'test':
            test_user_items = self.test_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()
        else:
            test_user_items = self.valid_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()
        test_user_items_dict = dict(zip(test_user_items['user_id'],test_user_items['item_id']))

        test_users = []
        test_items = []
        test_history = []
        test_history2 = []
        test_labels = []

        train_basket_items_dict['null'] = []
        for c,user in tqdm(enumerate(test_user_items_dict)):
            if user not in train_user_baskets_dict:
                continue
            if c % 100 ==1:
                print(c , 'user passed')
                #break

            baskets = train_user_baskets_dict[user]
            item_seq = {}
            for i, basket in enumerate(baskets):
                for item in train_basket_items_dict[basket]:
                    if item not in self.item_id_mapper:
                        continue
                    if item not in item_seq:
                        item_seq[item] = []
                    item_seq[item].append(i)


            label_items = test_user_items_dict[user]

            items = list(set(train_user_items_dict[user]))

            #print(len(history_baskets))
            for item in items:#train_user_items_dict[user]:
                if item not in self.item_id_mapper:
                    continue
                input_history = item_seq[item][-self.history_len:]
                if len(input_history) ==0:
                    continue
                if len(input_history) ==1 and input_history[0]==-1:
                    continue
                while len(input_history) < self.history_len:
                    input_history.insert(0,-1)

                real_input_history2 = []
                for j,x in enumerate(input_history[:-1]):
                    if x == -1:
                        real_input_history2.append(0)
                    else:
                        real_input_history2.append(input_history[j+1]-input_history[j])
                real_input_history2.append(len(baskets)-input_history[-1])
                test_users.append(self.user_id_mapper[user])
                test_items.append(self.item_id_mapper[item])
                    
                #test_history.append(real_input_history)
                test_history2.append(real_input_history2)
                test_labels.append(float(item in label_items))

        test_items = np.array(test_items)
        test_users = np.array(test_users)
        #test_history = np.array(test_history)
        test_history2 = np.array(test_history2)
        test_labels = np.array(test_labels)

        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_items.npy',test_items)
        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_users.npy',test_users)
       
        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_history2.npy',test_history2)
        #sparse.save_npz('Downloads/dunnhumby/'+ str(self.history_len) + '_'+test_data+'_history2.npz', test_history2)
        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_labels.npy',test_labels)

        return test_items, test_users, test_history2, test_labels

path_train = '/content/drive/MyDrive/insta_baskets_30k/train_baskets_30k.csv'  # для файлов что выше сделались
path_val = '/content/drive/MyDrive/insta_baskets_30k/valid_baskets_30k.csv'
path_test = '/content/drive/MyDrive/insta_baskets_30k/test_baskets_30k.csv'

dataset = PreDataset(path_train,path_val, path_test, dataset=dataset_name, history_len=history_len,basket_count_min=3, min_item_count = 5)

from google.colab import drive
drive.mount('/content/drive/')

history_len=20
item_embed_size=128
user_embed_size=32

h1 = 128
h2 = 128
h3 = 128
h4 = 128
h5 = 128
dataset_name = 'du'

dataset = PreDataset(path_train,path_val, path_test, dataset=dataset_name, history_len=history_len,basket_count_min=3, min_item_count = 5)

train_baskets = pd.read_csv(path_train)
test_baskets = pd.read_csv(path_test)
valid_baskets = pd.read_csv(path_val)

test_users = dataset.test_users

max_bucket_len = max(train_baskets.groupby('basket_id').item_id.count())
max_num_baskets = max(train_baskets.groupby('user_id').basket_id.nunique())

def gp_topfreq(train, test_users, n=max_num_baskets):
    result = {}
    top_popular = train.item_id.value_counts().index.tolist()[:max_bucket_len]
    train = train.sort_values(by='order_number')
    
    for user in tqdm(test_users):
        items = train[train.user_id==user]
        dates = items.order_number.unique()[-n:]
        top_personal = items[items.order_number.isin(dates)].item_id.value_counts().index.tolist()[:max_bucket_len]
        
        if len(top_personal)<max_bucket_len:
            gp_top = (top_personal + top_popular)[:max_bucket_len]
            result[user] = gp_top
        else:
            result[user] = top_personal
        
    return result

def p_topfreq(train, test_users, n=max_num_baskets):
    result = {}
    top_popular = train.item_id.value_counts().index.tolist()[:max_bucket_len]
    train = train.sort_values(by='date')
    
    for user in tqdm(test_users):
        items = train[train.user_id==user]
        dates = items.date.unique()[-n:]
        top_personal = items[items.date.isin(dates)].item_id.value_counts().index.tolist()[:max_bucket_len]
        
        # if len(top_personal)<max_bucket_len:
        #     gp_top = (top_personal + top_popular)[:max_bucket_len]
        #     result[user] = gp_top
        # else:
        result[user] = top_personal
        
    return result

res = p_topfreq(train_baskets, test_users, n=50)

res = gp_topfreq(train_baskets, test_users, n=50)

test_baskets = pd.read_csv(path_test)
user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()
user_test_baskets_dict = dict(zip( user_test_baskets_df['user_id'],user_test_baskets_df['item_id']))

user_predictions = res
final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))
print('predictions ready',len(user_predictions))
print('number of final test users:',len(final_users))
for k in [5,10,20,'B']:
    print(k)
    recall_scores = {}
    ndcg_scores = {}
    #zero = 0
    for user in final_users:

        top_items = []
        if user in user_predictions:
            top_items = user_predictions[user]
        else:
            zero+=1

        if k == 'B':
            recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))
            ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))
        else:
            recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,k)
            ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,k)
    #print(zero)
    print('recall:',np.mean(list(recall_scores.values())))
    print('ndcg:',np.mean(list(ndcg_scores.values())))