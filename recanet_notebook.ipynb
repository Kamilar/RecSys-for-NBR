{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfzLTcwaE9eM"
      },
      "outputs": [],
      "source": [
        "#from metrics import recall_k, ndcg_k, repeat_score_item, repeat_score_user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMEOu26D0gSC",
        "outputId": "dec4a8d0-f5d7-4374-8ae3-5891262d13bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmI0vcDQ0gSC",
        "outputId": "ee71e4ba-fab8-40bb-fc12-b128f759ae8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 23 05:50:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awzhwLd2SxXm"
      },
      "outputs": [],
      "source": [
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3GwkQhrSxU1"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key='4da15e357372a40c1c71a28c87306f19bf380d80')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaQpzVq_WT9n"
      },
      "outputs": [],
      "source": [
        "def wandb_start(config, run_name):\n",
        "    wandb.init(project=\"NBR_instacart\", config=config)\n",
        "    wandb.run.name = run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PwKoqUXfFI6"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C65AzIwS0gSD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import itertools\n",
        "import sys\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9pNJa4R0gSv"
      },
      "outputs": [],
      "source": [
        "basket_count_min = 3\n",
        "min_item_count = 5\n",
        "history_len = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyCRD_m50gSw"
      },
      "outputs": [],
      "source": [
        "# '''\n",
        "# Reads the raw files, renames columns, last basket as test and the rest as train.\n",
        "# No additional preprocessing steps.\n",
        "# '''\n",
        "\n",
        "# input_file_path = '/kaggle/input/data-ok/transaction_data.csv'  # сюда пути куда этот файл, я его я прям скину с таким же названием\n",
        "# train_baskets_file_path = '/kaggle/working/train_baskets1.csv'  #  сюда тоже пути, это для новых файло, они сделаются\n",
        "# test_baskets_file_path = '/kaggle/working/test_baskets1.csv'    #  сюда тоже пути, это для новых файло, они сделаются\n",
        "# valid_baskets_file_path = '/kaggle/working/valid_baskets1.csv'  #  сюда тоже пути, это для новых файло, они сделаются\n",
        "\n",
        "# df = pd.read_csv(input_file_path)\n",
        "# print(df.shape)\n",
        "# df['date'] = df['DAY'].astype(int)\n",
        "# df['basket_id'] = df['BASKET_ID']\n",
        "# df['item_id'] = df['PRODUCT_ID'].astype(str)\n",
        "# df['user_id'] = df['household_key'].astype(str)\n",
        "\n",
        "# processed_df = df[['date','basket_id','user_id','item_id']].drop_duplicates()\n",
        "# print(processed_df.shape)\n",
        "# print(processed_df.nunique())\n",
        "# last_baskets = processed_df[['user_id','basket_id','date']].drop_duplicates() \\\n",
        "#     .groupby('user_id').apply(lambda grp: grp.nlargest(1, 'date'))\n",
        "# last_baskets.index = last_baskets.index.droplevel()\n",
        "# test_baskets = pd.merge(last_baskets, processed_df, how='left')\n",
        "\n",
        "\n",
        "# train_baskets = pd.concat([processed_df,test_baskets]).drop_duplicates(keep=False)\n",
        "\n",
        "# all_users = list(set(test_baskets['user_id'].tolist()))\n",
        "# valid_indices = np.random.choice(range(len(all_users)),int(0.5*len(all_users)),\n",
        "#                                  replace=False)\n",
        "# valid_users = [all_users[i] for i in valid_indices]\n",
        "\n",
        "# valid_baskets = test_baskets[test_baskets['user_id'].isin(valid_users)]\n",
        "# test_baskets = test_baskets[~test_baskets['user_id'].isin(valid_users)]\n",
        "\n",
        "# print(valid_baskets.shape)\n",
        "# print(test_baskets.shape)\n",
        "# print(train_baskets.shape)\n",
        "\n",
        "# print(valid_baskets.nunique())\n",
        "# print(test_baskets.nunique())\n",
        "# print(train_baskets.nunique())\n",
        "\n",
        "# train_baskets.to_csv(train_baskets_file_path,index=False)\n",
        "# test_baskets.to_csv(test_baskets_file_path,index=False)\n",
        "# valid_baskets.to_csv(valid_baskets_file_path,index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF0GQZH30gSx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EjleLN0gSx"
      },
      "outputs": [],
      "source": [
        "# multihot_dict = {}\n",
        "# for user_id, items_list in zip(train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['user_id'],\n",
        "#                                train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['item_id']):\n",
        "#     l1 = sorted(list(set([dataset.item_id_mapper[int(item)] for item in items_list if int(item) in dataset.item_id_mapper])))\n",
        "#     ll2 = np.zeros(dataset.num_items-1)\n",
        "#     ll2[[(i - 1) for i in l1]] = 1\n",
        "#     multihot_dict[dataset.user_id_mapper[int(user_id)]] = ll2\n",
        "    #[1 if i in l1 else 0 for i in np.array(list(dataset.id_item_mapper.keys()))]\n",
        "    #print(len(list(multihot_dict.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwVZo9iR0gSx"
      },
      "outputs": [],
      "source": [
        "# multihot_dict = {}\n",
        "# for user_id, items_list in zip(train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['user_id'],\n",
        "#                                train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['item_id']):\n",
        "#     l1 = sorted(list(set([dataset.item_id_mapper[item] for item in items_list if item in dataset.item_id_mapper])))\n",
        "#     multihot_dict[dataset.user_id_mapper[user_id]] = [1 if i in l1 else 0 for i in np.array(list(dataset.id_item_mapper.keys()))]\n",
        "#     print(len(list(multihot_dict.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgR73qh8bzyD",
        "outputId": "baa66118-ffe2-4022-da2a-4ee1fbc4e817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2FXJH3S0gSy"
      },
      "outputs": [],
      "source": [
        "# preparind dataset for torch.dataset class\n",
        "class PreDataset():\n",
        "    def __init__(self, path_train,path_val, path_test, dataset, history_len, basket_count_min=0,\\\n",
        "                                                         min_item_count = 0):\n",
        "        self.basket_count_min = basket_count_min\n",
        "        self.min_item_count = min_item_count\n",
        "        self.history_len = history_len\n",
        "        \n",
        "        self.train_baskets = pd.read_csv(path_train)\n",
        "        self.valid_baskets = pd.read_csv(path_val)\n",
        "        self.test_baskets = pd.read_csv(path_test)\n",
        "        \n",
        "        basket_per_user = self.train_baskets[['user_id','basket_id']].drop_duplicates() \\\n",
        "            .groupby('user_id').agg({'basket_id':'count'}).reset_index()\n",
        "        self.test_users = basket_per_user[basket_per_user['basket_id'] >= self.basket_count_min]['user_id'].tolist()\n",
        "    \n",
        "        print(\"number of test users:\", len(self.test_users))\n",
        "        \n",
        "        self.model_name = 'data/dunnhumby_cj/'+dataset+ '_recanet'\n",
        "        self.dataset = dataset\n",
        "        self.all_items = self.train_baskets[['item_id']].drop_duplicates()['item_id'].tolist()\n",
        "        self.all_users = self.train_baskets[['user_id']].drop_duplicates()['user_id'].tolist()\n",
        "        self.num_items = len(self.all_items) +1\n",
        "        self.num_users = len(self.all_users) +1\n",
        "\n",
        "        print(\"items:\", self.num_items)\n",
        "        item_counts = self.train_baskets.groupby(['item_id']).size().to_frame(name = 'item_count').reset_index()\n",
        "        item_counts = item_counts[item_counts['item_count']>= min_item_count]\n",
        "        item_counts_dict = dict(zip(item_counts['item_id'],item_counts['item_count']))\n",
        "        print(\"filtered items:\", len(item_counts_dict))\n",
        "        self.num_items = len(item_counts_dict) +1\n",
        "        self.item_id_mapper = {} \n",
        "        self.id_item_mapper = {}\n",
        "        self.user_id_mapper = {}\n",
        "        self.id_user_mapper = {}\n",
        "\n",
        "        counter = 0\n",
        "        for i in range(len(self.all_items)):\n",
        "            if self.all_items[i] in item_counts_dict:\n",
        "                self.item_id_mapper[self.all_items[i]] = counter+1\n",
        "                self.id_item_mapper[counter+1] = self.all_items[i]\n",
        "                counter+=1\n",
        "        for i in range(len(self.all_users)):\n",
        "            self.user_id_mapper[self.all_users[i]] = i+1\n",
        "            self.id_user_mapper[i+1] = self.all_users[i]\n",
        "            \n",
        "            \n",
        "    def create_train_data(self):\n",
        "        #if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(20) + '_train_users.npy'):\n",
        "        if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(20) + '_train_users.npy'):\n",
        "            print('Data allready in use')\n",
        "            train_users = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_users.npy')\n",
        "            train_items = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_items.npy')\n",
        "      \n",
        "            #train_history2 = np.load('Downloads/dunnhumby3/dunnhumby_upd/'+ str(self.history_len) + '_train_history2.npy')\n",
        "            train_history2 = sparse.load_npz('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_train_history2.npz')\n",
        "            train_labels = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len)+ '_train_labels.npy')\n",
        "        #if os.path.isfile('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20_test_history2.npy'):\n",
        "        #if os.path.isfile('/kaggle/input/npy-files/dunnhumby_cj_recanet_20_test_history2.npy'):\n",
        "        #if os.path.isfile(self.model_name +'_' + str(self.history_len) + '_train_users.npy'):\n",
        "        #    print('Data allready in use')\n",
        "        #    train_users = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_train_users.npy')\n",
        "        #    train_items = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_train_items.npy')\n",
        "        #    #train_history = np.load(self.model_name +'_' + str(self.history_len) + '_train_history.npy')\n",
        "        #    train_history2 = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_train_history2.npy')\n",
        "        #    train_labels = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20'+ '_train_labels.npy')\n",
        "            return train_items,train_users, train_history2, train_labels\n",
        "        #if os.path.isfile(self.model_name +'_' + str(self.history_len) + '_train_users.npy'):\n",
        "        #    print('Data allready in use')\n",
        "        #    train_users = np.load(self.model_name +'_' + str(self.history_len) + '_train_users.npy')\n",
        "        #    train_items = np.load(self.model_name +'_' + str(self.history_len) + '_train_items.npy')\n",
        "      \n",
        "        #    train_history2 = np.load(self.model_name +'_' + str(self.history_len) + '_train_history2.npy')\n",
        "        #    train_labels = np.load(self.model_name +'_' + str(self.history_len)+ '_train_labels.npy')\n",
        "        #    return train_items,train_users, train_history2 , train_labels\n",
        "        row_counts = 0\n",
        "        basket_items = self.train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()\n",
        "        basket_items_dict = dict(zip(basket_items['basket_id'],basket_items['item_id']))\n",
        "        basket_items_dict['null'] = []\n",
        "\n",
        "        user_baskets = self.train_baskets[['user_id','date','basket_id']].drop_duplicates().\\\n",
        "            sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()\n",
        "\n",
        "        user_baskets_dict = dict(zip(user_baskets['user_id'],user_baskets['basket_id']))\n",
        "\n",
        "\n",
        "        train_users = []\n",
        "        train_items = []\n",
        "        train_history = []\n",
        "        train_history2 = []\n",
        "        train_labels = []\n",
        "        print('num users:', len(self.test_users))\n",
        "        \n",
        "        for c, user in tqdm(enumerate(self.test_users)):\n",
        "            if c % 1000 ==1:\n",
        "                print(c , 'user passed')\n",
        "\n",
        "            baskets = user_baskets_dict[user]\n",
        "            item_seq = {}\n",
        "            for i, basket in enumerate(baskets):\n",
        "                for item in basket_items_dict[basket]:\n",
        "                    if item not in self.item_id_mapper:\n",
        "                        continue\n",
        "                    if item not in item_seq:\n",
        "                        item_seq[item] = []\n",
        "                    item_seq[item].append(i)\n",
        "\n",
        "\n",
        "            for i in range(max(0,len(baskets)-50), len(baskets)):\n",
        "                label_basket = baskets[i]\n",
        "                all_history_baskets = baskets[:i]\n",
        "                items = []\n",
        "                for basket in all_history_baskets:\n",
        "                    for item in basket_items_dict[basket]:\n",
        "                        items.append(item)\n",
        "                items = list(set(items))\n",
        "                for item in items:\n",
        "                    if item not in self.item_id_mapper:\n",
        "                        continue\n",
        "                    index = np.argmax(np.array(item_seq[item])>=i)\n",
        "                    if np.max(np.array(item_seq[item])) < i:\n",
        "                        index = len(item_seq[item])\n",
        "                    input_history = item_seq[item][:index].copy()\n",
        "                    if len(input_history) ==0:\n",
        "                        continue\n",
        "                    if len(input_history) ==1 and input_history[0]==-1:\n",
        "                        continue\n",
        "                    while len(input_history) < self.history_len:\n",
        "                        input_history.insert(0,-1)\n",
        "                    #real_input_history = []\n",
        "                    #for x in input_history:\n",
        "                    #    if x == -1:\n",
        "                    #        real_input_history.append(0)\n",
        "                    #    else:\n",
        "                    #        real_input_history.append(i-x)\n",
        "                    real_input_history2 = []\n",
        "                    for j,x in enumerate(input_history[:-1]):\n",
        "                        if x == -1:\n",
        "                            real_input_history2.append(0)\n",
        "                        else:\n",
        "                            real_input_history2.append(input_history[j+1]-input_history[j])\n",
        "                    real_input_history2.append(i-input_history[-1])\n",
        "                    train_users.append(self.user_id_mapper[user])\n",
        "                    train_items.append(self.item_id_mapper[item])\n",
        "                    \n",
        "#                     real = csr_matrix(real_input_history2[-self.history_len:])\n",
        "#                     if type(train_history2) == list and len(train_history2) < 24721:\n",
        "#                         train_history2.append(real)\n",
        "#                     elif type(train_history2) == list and len(train_history2) == 24721:\n",
        "#                         train_history2 = vstack(train_history2)\n",
        "#                         train_history2 = vstack((train_history2, real))\n",
        "#                     else:\n",
        "#                         train_history2 = vstack((train_history2, real))\n",
        "                        \n",
        "                    #train_history.append(real_input_history[-self.history_len:])\n",
        "                    train_history2.append(real_input_history2[-self.history_len:])\n",
        "                    #print(item, basket_items_dict[label_basket])\n",
        "                    train_labels.append(float(item in basket_items_dict[label_basket]))\n",
        "\n",
        "                    row_counts +=1\n",
        "            print(row_counts)\n",
        "\n",
        "        train_items = np.array(train_items)\n",
        "        train_users = np.array(train_users)\n",
        "        #train_history = np.array(train_history)\n",
        "        train_history2 = np.array(train_history2)\n",
        "        train_labels = np.array(train_labels)\n",
        "        random_indices = np.random.choice(range(len(train_items)), len(train_items),replace=False)\n",
        "        train_items = train_items[random_indices]\n",
        "        train_users = train_users[random_indices]\n",
        "        #train_history = train_history[random_indices]\n",
        "        train_history2 = train_history2[random_indices]\n",
        "        train_labels = train_labels[random_indices]  \n",
        "        \n",
        "        train_history2 = csr_matrix(train_history2)\n",
        "\n",
        "        #np.save(self.model_name +'_' + str(self.history_len) + '_train_items.npy',train_items)\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_items.npy', train_items)\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_users.npy', train_users)\n",
        "      \n",
        "       # np.save('Downloads/dunn/'+ str(self.history_len) + '_train_history2.npy', train_history2)\n",
        "        sparse.save_npz('Downloads/dunn/'+ str(self.history_len) + '_train_history2.npz', train_history2)\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_train_labels.npy', train_labels)\n",
        "\n",
        "        return train_items,train_users, train_history2 , train_labels\n",
        "    \n",
        "    \n",
        "    def create_test_data(self,test_data='test'):\n",
        "        \n",
        "        if os.path.isfile('/content/drive/MyDrive/dunn2/'+ str(self.history_len)+ '_'+test_data+'_users.npy'):\n",
        "            test_users = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_users.npy')\n",
        "            test_items = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_items.npy')\n",
        "        \n",
        "            #test_history2 = sparse.load_npz('Downloads/dunnhumby3/'+ str(self.history_len) + '_'+test_data+'_history2.npz')\n",
        "            test_history2 = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_history2.npy')\n",
        "            test_labels = np.load('/content/drive/MyDrive/dunn2/'+ str(self.history_len) + '_'+test_data+'_labels.npy')\n",
        "        #if os.path.isfile('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20_test_history2.npy'):\n",
        "        #if os.path.isfile('/kaggle/input/npy-files/dunnhumby_cj_recanet_20_test_history2.npy'):\n",
        "        #if os.path.isfile(self.model_name +'_' + str(self.history_len)+ '_'+test_data+'_users.npy'):\n",
        "        #    test_users = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_'+test_data+'_users.npy')\n",
        "        #    test_items = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_'+test_data+'_items.npy')\n",
        "            #test_history = np.load(self.model_name +'_' + str(self.history_len) + '_'+test_data+'_history.npy')\n",
        "        #    test_history2 = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_'+test_data+'_history2.npy')\n",
        "        #    test_labels = np.load('/content/drive/MyDrive/dunnhumby_cj/dunnhumby_cj_recanet_20' + '_'+test_data+'_labels.npy')\n",
        "            return test_items,test_users,test_history2, test_labels\n",
        "        #if os.path.isfile(self.model_name +'_' + str(self.history_len)+ '_'+test_data+'_users.npy'):\n",
        "        #    test_users = np.load(self.model_name +'_' + str(self.history_len) + '_'+test_data+'_users.npy')\n",
        "        #    test_items = np.load(self.model_name +'_' + str(self.history_len) + '_'+test_data+'_items.npy')\n",
        "        \n",
        "        #    test_history2 = np.load(self.model_name +'_' + str(self.history_len) + '_'+test_data+'_history2.npy')\n",
        "        #    test_labels = np.load(self.model_name +'_' + str(self.history_len) + '_'+test_data+'_labels.npy')\n",
        "        #    return test_items, test_users,  test_history2, test_labels\n",
        "\n",
        "        train_basket_items = self.train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()\n",
        "        train_basket_items_dict = dict(zip(train_basket_items['basket_id'],train_basket_items['item_id']))\n",
        "\n",
        "        train_user_baskets = self.train_baskets[['user_id','date','basket_id']].drop_duplicates(). \\\n",
        "            sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()\n",
        "        train_user_baskets_dict = dict(zip(train_user_baskets['user_id'],train_user_baskets['basket_id']))\n",
        "\n",
        "        train_user_items = self.train_baskets[['user_id','item_id']].drop_duplicates().groupby(['user_id'])['item_id'] \\\n",
        "            .apply(list).reset_index()\n",
        "        train_user_items_dict = dict(zip(train_user_items['user_id'],train_user_items['item_id']))\n",
        "\n",
        "        test_user_items = None\n",
        "        if test_data == 'test':\n",
        "            test_user_items = self.test_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()\n",
        "        else:\n",
        "            test_user_items = self.valid_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()\n",
        "        test_user_items_dict = dict(zip(test_user_items['user_id'],test_user_items['item_id']))\n",
        "\n",
        "        test_users = []\n",
        "        test_items = []\n",
        "        test_history = []\n",
        "        test_history2 = []\n",
        "        test_labels = []\n",
        "\n",
        "        train_basket_items_dict['null'] = []\n",
        "        for c,user in tqdm(enumerate(test_user_items_dict)):\n",
        "            if user not in train_user_baskets_dict:\n",
        "                continue\n",
        "            if c % 100 ==1:\n",
        "                print(c , 'user passed')\n",
        "                #break\n",
        "\n",
        "            baskets = train_user_baskets_dict[user]\n",
        "            item_seq = {}\n",
        "            for i, basket in enumerate(baskets):\n",
        "                for item in train_basket_items_dict[basket]:\n",
        "                    if item not in self.item_id_mapper:\n",
        "                        continue\n",
        "                    if item not in item_seq:\n",
        "                        item_seq[item] = []\n",
        "                    item_seq[item].append(i)\n",
        "\n",
        "\n",
        "            label_items = test_user_items_dict[user]\n",
        "\n",
        "            items = list(set(train_user_items_dict[user]))\n",
        "\n",
        "            #print(len(history_baskets))\n",
        "            for item in items:#train_user_items_dict[user]:\n",
        "                if item not in self.item_id_mapper:\n",
        "                    continue\n",
        "                input_history = item_seq[item][-self.history_len:]\n",
        "                if len(input_history) ==0:\n",
        "                    continue\n",
        "                if len(input_history) ==1 and input_history[0]==-1:\n",
        "                    continue\n",
        "                while len(input_history) < self.history_len:\n",
        "                    input_history.insert(0,-1)\n",
        "                #real_input_history = []\n",
        "                #for x in input_history:\n",
        "                #    if x == -1:\n",
        "                #        real_input_history.append(0)\n",
        "                #    else:\n",
        "                #        real_input_history.append(len(baskets)-x)\n",
        "\n",
        "                real_input_history2 = []\n",
        "                for j,x in enumerate(input_history[:-1]):\n",
        "                    if x == -1:\n",
        "                        real_input_history2.append(0)\n",
        "                    else:\n",
        "                        real_input_history2.append(input_history[j+1]-input_history[j])\n",
        "                real_input_history2.append(len(baskets)-input_history[-1])\n",
        "                test_users.append(self.user_id_mapper[user])\n",
        "                test_items.append(self.item_id_mapper[item])\n",
        "                    \n",
        "                #test_history.append(real_input_history)\n",
        "                test_history2.append(real_input_history2)\n",
        "                test_labels.append(float(item in label_items))\n",
        "\n",
        "        test_items = np.array(test_items)\n",
        "        test_users = np.array(test_users)\n",
        "        #test_history = np.array(test_history)\n",
        "        test_history2 = np.array(test_history2)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_items.npy',test_items)\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_users.npy',test_users)\n",
        "       \n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_history2.npy',test_history2)\n",
        "        #sparse.save_npz('Downloads/dunnhumby/'+ str(self.history_len) + '_'+test_data+'_history2.npz', test_history2)\n",
        "        np.save('Downloads/dunn/'+ str(self.history_len) + '_'+test_data+'_labels.npy',test_labels)\n",
        "\n",
        "        return test_items, test_users, test_history2, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgG-b0m-0gS1"
      },
      "outputs": [],
      "source": [
        "# path_train = f'/kaggle/input/dunnhumby-dataset/train_baskets.csv'\n",
        "# path_test = f'/kaggle/input/dunnhumby-dataset/test_baskets.csv'\n",
        "# path_val = f'/kaggle/input/dunnhumby-dataset/valid_baskets.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3iv5CDPAGug"
      },
      "outputs": [],
      "source": [
        "class RCNDataset(Dataset):\n",
        "    def __init__(self, model, mode):\n",
        "\n",
        "        self.mode = mode\n",
        "        if self.mode=='train':\n",
        "            self.x1, self.x2, self.x4, self.y = model.create_train_data()\n",
        "        elif self.mode=='valid':\n",
        "            self.x1, self.x2, self.x4, self.y = model.create_test_data(test_data='valid')\n",
        "        elif self.mode=='test':\n",
        "            self.x1, self.x2, self.x4, self.y = model.create_test_data(test_data='test')\n",
        "        else: \n",
        "            print('Mode error')\n",
        "\n",
        "        #self.x1, self.x2, self.x3, self.x4, self.y = torch.tensor(self.x1).long(), torch.tensor(self.x2).long(),\\\n",
        "        #            torch.tensor(self.x3), torch.tensor(self.x4), torch.tensor(self.y)\n",
        "        if self.mode == 'train':\n",
        "            self.x1, self.x2, self.y = torch.tensor(self.x1).long(), torch.tensor(self.x2).long(),\\\n",
        "              torch.FloatTensor(self.y)\n",
        "        else:\n",
        "            self.x1, self.x2, self.x4, self.y = torch.tensor(self.x1).long(), torch.tensor(self.x2).long(),\\\n",
        "                     torch.FloatTensor(self.x4), torch.FloatTensor(self.y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "            return self.x1[idx], self.x2[idx], torch.FloatTensor(self.x4[idx].toarray()), self.y[idx]\n",
        "        else:\n",
        "            return self.x1[idx], self.x2[idx], self.x4[idx], self.y[idx]\n",
        "\n",
        "## loading to device\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85Ys0Q_MAzAq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzzFkFxaMMj7"
      },
      "outputs": [],
      "source": [
        "history_len=20\n",
        "item_embed_size=128\n",
        "user_embed_size=32\n",
        "\n",
        "h1 = 128\n",
        "h2 = 128\n",
        "h3 = 128\n",
        "h4 = 128\n",
        "h5 = 128\n",
        "dataset_name = 'du'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j309PMEZFWhO"
      },
      "outputs": [],
      "source": [
        "#!python -m wget -nc https://www.dropbox.com/s/5aac9i8z5uiewrn/transaction_data.csv.zip?dl=0 -O transaction_data.zip\n",
        "# if the link does not work, you can download the dataset manually from https://disk.yandex.ru/d/nuyCNaDrE1Bq0w\n",
        "#!python -m unzip -n transaction_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWlIoHIhH9o8"
      },
      "outputs": [],
      "source": [
        "#!python create_dunnhumby_cj_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q56K3TV0gS3"
      },
      "outputs": [],
      "source": [
        "# path_train = '/kaggle/working/train_baskets1.csv'  # для файлов что выше сделались\n",
        "# path_val = '/kaggle/working/test_baskets1.csv'\n",
        "# path_test = '/kaggle/working/valid_baskets1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb4CoRw30gS3"
      },
      "outputs": [],
      "source": [
        "path_train = '/content/drive/MyDrive/baskets/train_baskets-git.csv'  # для файлов что выше сделались\n",
        "path_val = '/content/drive/MyDrive/baskets/valid_baskets-git.csv'\n",
        "path_test = '/content/drive/MyDrive/baskets/test_baskets-git.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDP2IlfbHXlL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycHZRfMdHXn4"
      },
      "outputs": [],
      "source": [
        "# path_train = '/content/drive/MyDrive/instacart_baskets/train_baskets_inst.csv'  # для файлов что выше сделались\n",
        "# path_val = '/content/drive/MyDrive/instacart_baskets/valid_baskets_inst.csv'\n",
        "# path_test = '/content/drive/MyDrive/instacart_baskets/test_baskets_inst.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk1QMees0gS3",
        "outputId": "0ef4fc44-2732-43a5-fd03-6749511f4a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of test users: 2483\n",
            "items: 91764\n",
            "filtered items: 36963\n"
          ]
        }
      ],
      "source": [
        "dataset = PreDataset(path_train,path_val, path_test, dataset=dataset_name, history_len=history_len,basket_count_min=3, min_item_count = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsmgdNZh0gS3"
      },
      "outputs": [],
      "source": [
        "train_baskets = pd.read_csv(path_train)\n",
        "test_baskets = pd.read_csv(path_test)\n",
        "valid_baskets = pd.read_csv(path_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfPXXdRt0gS4",
        "outputId": "b80ed71a-88a5-49f9-a255-df8d8d7cca09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15372, 4), (14099, 4), (2566261, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_baskets.shape, valid_baskets.shape, train_baskets.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_users = dataset.test_users"
      ],
      "metadata": {
        "id": "aiF7X69cNj2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC4nATxNNvpr",
        "outputId": "fbd5eb94-b31d-421d-ca67-b1afa174e837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2483"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFBRG_O-Nvmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_bucket_len = max(train_baskets.groupby('basket_id').item_id.count())\n",
        "max_num_baskets = max(train_baskets.groupby('user_id').basket_id.nunique())"
      ],
      "metadata": {
        "id": "bj93ul3INjzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gp_topfreq(train, test_users, n=max_num_baskets):\n",
        "    result = {}\n",
        "    top_popular = train.item_id.value_counts().index.tolist()[:max_bucket_len]\n",
        "    train = train.sort_values(by='date')\n",
        "    \n",
        "    for user in tqdm(test_users):\n",
        "        items = train[train.user_id==user]\n",
        "        dates = items.date.unique()[-n:]\n",
        "        top_personal = items[items.date.isin(dates)].item_id.value_counts().index.tolist()[:max_bucket_len]\n",
        "        \n",
        "        if len(top_personal)<max_bucket_len:\n",
        "            gp_top = (top_personal + top_popular)[:max_bucket_len]\n",
        "            result[user] = gp_top\n",
        "        else:\n",
        "            result[user] = top_personal\n",
        "        \n",
        "    return result"
      ],
      "metadata": {
        "id": "B3Ps7R_CORth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = gp_topfreq(train_baskets, test_users, n=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "38c8939aa01b467b8e966c3fbad6c89c",
            "a1ab8d7c2425404db947bf004497d39f",
            "7aaa3fc7cd1842e3983017906fe58447",
            "0a28ee35fd4e406f81f0e16ec0163674",
            "085ae608526545ee9659467431ef45b1",
            "4ca1f9596a6f4fd9a0f7955699da9ba2",
            "725325da0f7943f7b8ac9b4a9227ca37",
            "ac7b075a5fe24518ac1d9771b98c729d",
            "9e0564aabfb94d8d9a44f188d0ac17eb",
            "584e71fb7a8a43498e21d8f84b4b309c",
            "bb648c7e29764eb491164490e2258741"
          ]
        },
        "id": "zh3Ll7suORqR",
        "outputId": "6f30051a-e443-47a4-fb7c-329d9d875f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2483 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38c8939aa01b467b8e966c3fbad6c89c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mO06q9GMOhmv",
        "outputId": "7ac9a4fe-6445-43bb-d5ca-ab69dad97ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/baskets/test_baskets-git.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_baskets = pd.read_csv(path_test)\n",
        "user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
        "user_test_baskets_dict = dict(zip( user_test_baskets_df['user_id'],user_test_baskets_df['item_id']))\n",
        "\n",
        "user_predictions = res\n",
        "final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))\n",
        "print('predictions ready',len(user_predictions))\n",
        "print('number of final test users:',len(final_users))\n",
        "for k in [5,10,20,'B']:\n",
        "    print(k)\n",
        "    recall_scores = {}\n",
        "    ndcg_scores = {}\n",
        "    #zero = 0\n",
        "    for user in final_users:\n",
        "\n",
        "        top_items = []\n",
        "        if user in user_predictions:\n",
        "            top_items = user_predictions[user]\n",
        "        else:\n",
        "            zero+=1\n",
        "\n",
        "        if k == 'B':\n",
        "            recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "            ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "        else:\n",
        "            recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,k)\n",
        "            ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,k)\n",
        "    #print(zero)\n",
        "    print('recall:',np.mean(list(recall_scores.values())))\n",
        "    print('ndcg:',np.mean(list(ndcg_scores.values())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cFaZGQROgRM",
        "outputId": "4463a7e8-aff4-4906-e726-43e8730facfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions ready 2483\n",
            "number of final test users: 1243\n",
            "5\n",
            "recall: 0.1122471587550507\n",
            "ndcg: 0.1710109936246384\n",
            "10\n",
            "recall: 0.14732749868705472\n",
            "ndcg: 0.14012473594393066\n",
            "20\n",
            "recall: 0.20017823183717073\n",
            "ndcg: 0.11050241486255191\n",
            "B\n",
            "recall: 0.12149068839735666\n",
            "ndcg: 0.13985002689423645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BogXqU-eOgMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "1UirDmnLOZjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlaUarlvOZfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOFD00wTNjxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPQJzgODNjtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52uYlYMC0gS4"
      },
      "outputs": [],
      "source": [
        "multihot_dict = {}\n",
        "for user_id, items_list in zip(train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['user_id'],\n",
        "                               train_baskets.groupby(['user_id'])['item_id'].apply(list).reset_index()['item_id']):\n",
        "    l1 = sorted(list(set([dataset.item_id_mapper[int(item)] for item in items_list if int(item) in dataset.item_id_mapper])))\n",
        "    ll2 = np.zeros(dataset.num_items-1)\n",
        "    ll2[[(i - 1) for i in l1]] = 1\n",
        "    multihot_dict[dataset.user_id_mapper[int(user_id)]] = ll2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGtMMII3dYFQ",
        "outputId": "6f20db3b-33a8-41ef-dd05-dd3ad89cd26d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10240, 8192, 6144, 4096)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "2048*5, 2048*4, 2048*3, 2048*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo6h3P0xFUFU",
        "outputId": "977bd3c3-a601-492e-ec3c-05151fcf223a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((643538,), (643538,), (643538, 20), (643538,))"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1.shape, x2.shape, x4.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzNX89BkFT6j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYUec4ql0gS4",
        "outputId": "d88ac9a1-fa87-4ac7-9b1a-1b97b998a8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data allready in use\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2048\n",
        "\n",
        "train_dataset = RCNDataset(dataset, mode='train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = RCNDataset(dataset, mode='valid')\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset = RCNDataset(dataset, mode='test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P21yv3KT3H5",
        "outputId": "5b83b593-1bde-4c06-9ad1-d4e96c8c4ca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50176675, 643538, 658898)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Z3Ke2QH9RT",
        "outputId": "6709f9c3-0a64-4773-bf3e-1e9102640ab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(178201010, 6466341, 6511587)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-LT_zJHH-i4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gHFKTi0gS4"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, kq_same=False, bias=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        It has projection layer for getting keys, queries and values. Followed by attention.\n",
        "        \"\"\"\n",
        "        self.d_model = d_model\n",
        "        self.h = n_heads\n",
        "        self.d_k = self.d_model // self.h\n",
        "        self.kq_same = kq_same\n",
        "\n",
        "        if not kq_same:\n",
        "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
        "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "    def head_split(self, x):  # get dimensions bs * h * seq_len * d_k\n",
        "        new_x_shape = x.size()[:-1] + (self.h, self.d_k)\n",
        "        return x.view(*new_x_shape).transpose(-2, -3)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        origin_shape = q.size()\n",
        "\n",
        "        # perform linear operation and split into h heads\n",
        "        if not self.kq_same:\n",
        "            q = self.head_split(self.q_linear(q))\n",
        "        else:\n",
        "            q = self.head_split(self.k_linear(q))\n",
        "        k = self.head_split(self.k_linear(k))\n",
        "        v = self.head_split(self.v_linear(v))\n",
        "\n",
        "        # calculate attention using function we will define next\n",
        "        output = self.scaled_dot_product_attention(q, k, v, self.d_k, mask)\n",
        "\n",
        "        # concatenate heads and put through final linear layer\n",
        "        output = output.transpose(-2, -3).reshape(origin_shape)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def scaled_dot_product_attention(q, k, v, d_k, mask=None):\n",
        "        \"\"\"\n",
        "        This is called by Multi-head attention object to find the values.\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / d_k ** 0.5  # bs, head, q_len, k_len\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -np.inf)\n",
        "        scores = (scores - scores.max()).softmax(dim=-1)\n",
        "        scores = scores.masked_fill(torch.isnan(scores), 0)\n",
        "        output = torch.matmul(scores, v)  # bs, head, q_len, d_k\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCYSO9E80gS5"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, n_heads, dropout=0, kq_same=False):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        This is a Basic Block of Transformer. It contains one Multi-head attention object. \n",
        "        Followed by layer norm and position wise feedforward net and dropout layer.\n",
        "        \"\"\"\n",
        "        # Multi-Head Attention Block\n",
        "        self.masked_attn_head = MultiHeadAttention(d_model, n_heads, kq_same=kq_same)\n",
        "\n",
        "        # Two layer norm layer and two dropout layer\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, seq, mask=None):\n",
        "        context = self.masked_attn_head(seq, seq, seq, mask)\n",
        "        context = self.layer_norm1(self.dropout1(context) + seq)\n",
        "        output = self.linear1(context).relu()\n",
        "        output = self.linear2(output)\n",
        "        output = self.layer_norm2(self.dropout2(output) + context)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71pxQINMeCn4"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(dim_in, dim_out)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        #self.norm = nn.BatchNorm2d(dim_out)\n",
        "        self.norm = nn.LayerNorm(dim_out, elementwise_affine = False)\n",
        "        #self.dropout = nn.Dropout(0.2)\n",
        "        self.act = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act(self.norm(self.dropout(self.dense(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqnfDY2kbkEy"
      },
      "outputs": [],
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=6, nhead=6, batch_first=True)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU0NMnNECTRH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZbhKINqDaDY",
        "outputId": "bdbbd1d1-85be-4e0f-c904-203cd3b01b51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6144, 128])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_encoder(x14).mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIc4PyUKDZxX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZSmh33tJMXk",
        "outputId": "8afa7104-d872-406d-eaa9-f96a8a2c6ab2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 6])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.eye(4, 6).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaWqEjZaJRqh",
        "outputId": "3bc7f0d6-bb0b-4afb-9694-a2cbd5405328"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 6])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_encoder(torch.eye(4, 6)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV_90VTuJMUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eoLJUqQ0gS5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ReCaNet(nn.Module):\n",
        "    def __init__(self, num_items, item_embed_size, num_users, user_embed_size, history_len, h1, h2, h3, h4, h5):\n",
        "        super(ReCaNet, self).__init__()\n",
        "        self.num_items = num_items\n",
        "        self.item_embed_size = item_embed_size\n",
        "        self.num_users = num_users\n",
        "        self.user_embed_size = user_embed_size\n",
        "        self.history_len = history_len\n",
        "        self.h1 = h1\n",
        "        self.h2 = h2\n",
        "        self.h3 = h3\n",
        "        self.h4 = h4\n",
        "        self.h5 = h5\n",
        "\n",
        "        self.item_embedding = nn.Embedding(self.num_items, self.item_embed_size)\n",
        "        self.user_embedding = nn.Embedding(self.num_users, self.user_embed_size)\n",
        "        \n",
        "        #self.fc1 = nn.Linear(self.item_embed_size + 36963, self.h1)\n",
        "            #self.item_embed_size + self.user_embed_size, self.h1)\n",
        "        self.fc1 = nn.Linear(self.item_embed_size + + self.user_embed_size, self.h1)\n",
        "        self.fc2 = nn.Linear(self.h1+1, self.h2)\n",
        "        self.lstm1 = nn.LSTM(self.h2, self.h3, batch_first=True, num_layers=2)\n",
        "        #self.lstm2 = nn.LSTM(self.h3, self.h4, batch_first=True)\n",
        "        self.fc5 = nn.Linear(self.h4, self.h5)\n",
        "        self.fc6 = nn.Linear(self.h4, self.h5)\n",
        "        self.fc7 = nn.Linear(self.h5, 1)\n",
        "        #self.transformer_block = nn.ModuleList([\n",
        "        #    TransformerLayer(d_model=self.h1, d_ff=self.h1, n_heads=4,\n",
        "        #                            dropout=0.1, kq_same=False)\n",
        "        #    for _ in range(1)\n",
        "        #])\n",
        "        #self.layers = nn.Sequential(\n",
        "        #        Block(36963, 2048), \n",
        "        #        Block(2048, 512), \n",
        "        #        Block(512, 128)\n",
        "        #)\n",
        "        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=16, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=8)\n",
        "\n",
        "    def forward(self, input1, input2, input4):\n",
        "        x1 = self.item_embedding(input1.to(torch.int64))\n",
        "        x1 = x1.view(-1, self.item_embed_size) #1x128\n",
        "       \n",
        "        x2 = self.user_embedding(input2.to(torch.int64))\n",
        "        #x2 = torch.tensor(np.array([multihot_dict[i.item()] for i in input2.to(torch.int64)])).long().to(torch.float32).to(device)\n",
        "        #x2 = x2.view(-1, 36963)\n",
        "                     #self.user_embed_size) #1x32\n",
        "        x2 = x2.view(-1, self.user_embed_size)\n",
        "        #x2 = self.layers(x2)\n",
        "        # 1x5\n",
        "        x4 = input4 # 1x5\n",
        "        x4 = x4.unsqueeze(2)\n",
        "        \n",
        "        conc1 = torch.cat((x1, x2), axis=1)\n",
        "        x11 = F.relu(self.fc1(conc1)) # 1*128\n",
        "\n",
        "        x12 = x11.unsqueeze(1).repeat(1, self.history_len, 1)\n",
        "        \n",
        "        conc2 = torch.cat((x12, x4), 2)#.transpose(-2,-1)\n",
        "        x14 = F.relu(self.fc2(conc2))\n",
        "        #valid_his = (input4 > 0).long()\n",
        "        \n",
        "        ###    ! Начало вставки с трансформером\n",
        "#         causality_mask = np.tril(np.ones((1, 1, 20, 20), dtype=np.int))\n",
        "#         attn_mask = torch.from_numpy(causality_mask).to(device)\n",
        "\n",
        "#         for block in self.transformer_block:\n",
        "#             his_vectors = block(x14, attn_mask)\n",
        "\n",
        "#         valid_his = (input4 > 0).long()\n",
        "#         seq_lengths = torch.sum(valid_his, axis=1)\n",
        "#         his_vectors = his_vectors * valid_his[:, :, None].float()\n",
        "\n",
        "#         his_vector = his_vectors[torch.arange(len(seq_lengths)), seq_lengths - 1, :]\n",
        "        \n",
        "#         x22 = his_vector[:, None, :].squeeze(1)\n",
        "\n",
        "        ###    ! Конец вставки с трансформером\n",
        "\n",
        "        x21, (hx, cx) = self.lstm1(x14)             #Прежняя часть с LSTM\n",
        "        x22 = hx[1]                                 #Прежняя часть с LSTM\n",
        "       \n",
        "        #x22 = self.transformer_encoder(x14).mean(dim=1)\n",
        "        x = F.relu(self.fc5(x22))\n",
        "        x = F.relu(self.fc6(x))\n",
        "      \n",
        "        output = self.fc7(x).view(-1) \n",
        "        output = torch.sigmoid(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCizQlat_2gO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng-YSzf7KObY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhG4zm4ZKOMf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slDVRLyK0gS5"
      },
      "outputs": [],
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model_new = ReCaNet(num_items=dataset.num_items, item_embed_size=item_embed_size, num_users=dataset.num_users, \n",
        "#                 user_embed_size=user_embed_size, history_len = history_len, h1 = h1,h2 = h2, h3 = h3,\n",
        "#                 h4 = h4, h5 = h5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEwTfHri0gS5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFW2YqZ0gS5"
      },
      "outputs": [],
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model_new = ReCaNet(num_items=dataset.num_items, item_embed_size=item_embed_size, num_users=dataset.num_users, \n",
        "#                 user_embed_size=user_embed_size, history_len = history_len, h1 = h1,h2 = h2, h3 = h3,\n",
        "#                 h4 = h4, h5 = h5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "762kh71g0gS6"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_new = ReCaNet(num_items=dataset.num_items, item_embed_size=item_embed_size, num_users=dataset.num_users, \n",
        "                user_embed_size=user_embed_size, history_len = history_len, h1 = h1,h2 = h2, h3 = h3,\n",
        "                h4 = h4, h5 = h5).to(device)\n",
        "\n",
        "#path_best = '/kaggle/input/weights/epoch_5_recanet_model_dunnhumby.pth'\n",
        "#model_new.load_state_dict(torch.load(path_best))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARwGebTY_bvo"
      },
      "outputs": [],
      "source": [
        "#optimizer = torch.optim.Adam(model_new.parameters(), lr = 0.001)\n",
        "#criterion = nn.BCELoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wktiHFCLattx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "BraqZCLqG8Ww",
        "outputId": "4a130da1-acd1-4d38-b100-cc9c5c38a750"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_073319-gbr05jiy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kamila/NBR_instacart/runs/gbr05jiy' target=\"_blank\">denim-grass-1</a></strong> to <a href='https://wandb.ai/kamila/NBR_instacart' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kamila/NBR_instacart' target=\"_blank\">https://wandb.ai/kamila/NBR_instacart</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kamila/NBR_instacart/runs/gbr05jiy' target=\"_blank\">https://wandb.ai/kamila/NBR_instacart/runs/gbr05jiy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb_start(config, 'recanet_instacart')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7UMh8K-gHaC",
        "outputId": "0b0e8fd6-7dde-4249-d9d8-d49e6ec451a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.watch(model_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlgVbG10gS7"
      },
      "outputs": [],
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmU6lM5x0gS8"
      },
      "outputs": [],
      "source": [
        "def epoch_recall(model, y_true ,y_pred, predictions, user_valid_baskets_dict, valid_users, valid_items):\n",
        "    recall_scores = []\n",
        "    for user in user_valid_baskets_dict:\n",
        "        top_items = []\n",
        "        if user in dataset.user_id_mapper:\n",
        "            user_id = dataset.user_id_mapper[user]\n",
        "            indices = np.argwhere(valid_users == user_id)\n",
        "            item_scores = np.array(y_pred)[indices].flatten()\n",
        "            item_ids = valid_items[indices].flatten()\n",
        "\n",
        "            item_score_dic = {}\n",
        "            for i, item_id in enumerate(item_ids):\n",
        "                item_score_dic[dataset.id_item_mapper[item_id]] = item_scores[i]\n",
        "            sorted_item_scores = sorted(item_score_dic.items(), key= lambda x: x[1], reverse = True)\n",
        "            top_items = [x[0] for x in sorted_item_scores]\n",
        "        recall_scores.append(recall_k(user_valid_baskets_dict[user],top_items,\n",
        "                                              len(user_valid_baskets_dict[user])))\n",
        "            \n",
        "    return np.mean(recall_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myHswTdDs7oc",
        "outputId": "4e2cd446-363a-44f1-9a0e-6f9dddc2863c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6144]), torch.Size([6144]), torch.Size([6144, 20]))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1.shape, x2.shape, x4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f504db3697184171ba2461e502ce6d17",
            "c0ccaff87cab491698d8c9ab95037fe2",
            "7302dfd81d334fa38f727853efb10ba5",
            "333057e4c4de41a1851269c7c46768cb",
            "baced45ab7af4f27b76cadef2d9b0cc3",
            "d65594118fa64f09bb9cfe9d86c32d97",
            "cf78050cb973474ab704684da63f9549",
            "752b52a1a5e24cd4beefc4734c9d93dd",
            "e7389940e5f84c34bd11978e94448b91",
            "8151a5ff0c774445a3e869417ae1c46a",
            "c722d319f9f24b569dc75c2456f56382"
          ]
        },
        "id": "dml6LvKu0gS8",
        "outputId": "15c6d5b3-40b8-4bcb-838e-61bb6e649699"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f504db3697184171ba2461e502ce6d17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8167 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for x1, x2, x4, y in tqdm(train_loader):\n",
        "    x1 = x1.to(device)\n",
        "    x2 = x2.to(device)\n",
        "    #x3 = x3.to(device)\n",
        "    x4 = x4.squeeze(1)\n",
        "    x4 = x4.to(device) \n",
        "\n",
        "    output, x14, x21, (hx, cx), valid_his = model_new(x1, x2, x4)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkcmNbMFtSrC",
        "outputId": "f65f323a-6fff-41bf-910e-686e0e7326fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6144]), torch.Size([6144, 20, 128]))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape, x14.shape, x21.shape, (hx, cx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmPI2TwjtSPq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZGdtjRK0gS9"
      },
      "outputs": [],
      "source": [
        "#output.shape, causality_mask.shape, attn_mask.shape, x14.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmUGtruE0gS9"
      },
      "outputs": [],
      "source": [
        "def train(model_new, epochs, calc_recall=True, checkpoint=False):\n",
        "    torch.cuda.empty_cache()\n",
        "    #clear_output(wait=True)\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    metric_val = []\n",
        "    metric_train = []\n",
        "\n",
        "    recall_score = []\n",
        "   \n",
        "    best_model = None\n",
        "    best_recall = 0\n",
        "\n",
        "    parameters = filter(lambda p: p.requires_grad, model_new.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
        "    criterion = nn.BCELoss(reduction='mean')\n",
        "\n",
        "    #validation\n",
        "    user_valid_baskets_df = dataset.valid_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
        "    user_valid_baskets_dict = dict(zip( user_valid_baskets_df['user_id'],user_valid_baskets_df['item_id']))\n",
        "    valid_items, valid_users, valid_history2 ,valid_labels = dataset.create_test_data('valid')\n",
        "\n",
        "    \n",
        "    for i in tqdm(range(epochs)):\n",
        "        print(f'Epoch {i}')\n",
        "        loss_train_epoch = []\n",
        "        loss_val_epoch = []\n",
        "        metric_val_epoch = []\n",
        "        metric_train_epoch = []\n",
        "\n",
        "        model_new.train()\n",
        "        for x1, x2, x4, y in tqdm(train_loader):\n",
        "            x1 = x1.to(device)\n",
        "            x2 = x2.to(device)\n",
        "            #x3 = x3.to(device)\n",
        "            x4 = x4.squeeze(1)\n",
        "            x4 = x4.to(device) \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            y_pred = model_new(x1, x2, x4)\n",
        "            predictions = [round(value) for value in y_pred.flatten().tolist()]\n",
        "        \n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            wandb.log({'train loss':loss.item()\n",
        "                        })\n",
        "            \n",
        "            loss_train_epoch.append(loss.item())\n",
        "            metric_train_epoch.append(accuracy_score(y.detach().cpu().numpy(), predictions))\n",
        "        \n",
        "        loss_train.append(np.mean(loss_train_epoch))\n",
        "        metric_train.append(np.mean(metric_train_epoch))\n",
        "        print(f'loss_train_epoch_{i}', np.mean(loss_train_epoch))\n",
        "        print(f'metric_train_epoch_{i}', np.mean(metric_train_epoch))\n",
        "\n",
        "        model_new.eval()\n",
        "        \n",
        "        full_y = []\n",
        "        full_predictions = []\n",
        "        pred_scores = []\n",
        "        for x1, x2,  x4, y in val_loader:\n",
        "            with torch.no_grad():\n",
        "                x1 = x1.to(device)\n",
        "                x2 = x2.to(device)\n",
        "                #x3 = x3.to(device)\n",
        "                x4 = x4.to(device) \n",
        "                y_pred = model_new(x1, x2, x4)\n",
        "                \n",
        "                loss = criterion(y_pred, y)\n",
        "                wandb.log({'test loss':loss.item()\n",
        "                        })\n",
        "\n",
        "                predictions = [round(value) for value in y_pred.flatten().tolist()]\n",
        "                full_y.extend(y.detach().cpu())\n",
        "                pred_scores.extend(y_pred.detach().cpu())\n",
        "                full_predictions.extend(predictions)\n",
        "\n",
        "                loss_val_epoch.append(loss.item())\n",
        "                metric_val_epoch.append(accuracy_score(y.detach().cpu().numpy(), predictions))\n",
        "              \n",
        "        print(f'loss_val_epoch_{i}', np.mean(loss_val_epoch))\n",
        "        print(f'metric_val_epoch_{i}', np.mean(metric_val_epoch))\n",
        "        accuracy = accuracy_score(full_y, full_predictions)\n",
        "        print(\"Accuracy epoch: %.2f%%\" % (accuracy * 100.0))\n",
        "        \n",
        "        if calc_recall:\n",
        "            recall_per_epoch = epoch_recall(model_new,full_y,pred_scores, full_predictions, user_valid_baskets_dict, \\\n",
        "                                             valid_users, valid_items)\n",
        "            recall_score.append(recall_per_epoch)\n",
        "\n",
        "            if recall_per_epoch > best_recall:\n",
        "                best_model = model_new\n",
        "                best_recall = recall_per_epoch\n",
        "                path_best = f'/kaggle/working/epoch_{i}_recanet_best_model_dunnhumby.pth'\n",
        "                # '/kaggle/working/train_baskets.csv'\n",
        "                #f'/content/drive/MyDrive/epoch_{i}_recanet_best_model_dunnhumby.pth'\n",
        "                torch.save(best_model.state_dict(), path_best) #saving model with best loss\n",
        "\n",
        "        best_model = model_new       \n",
        "        path_best = f'/content/drive/MyDrive/ckpt_instacart/epoch_{i}_recanet_model_inst.pth'\n",
        "        torch.save(model_new.state_dict(), path_best) #saving model with best loss\n",
        "\n",
        "        loss_val.append(np.mean(loss_val_epoch))\n",
        "        metric_val.append(np.mean(metric_val_epoch))\n",
        "        \n",
        "        # отрисовка графиков\n",
        "        #clear_output(wait=True)\n",
        "        #plot_training(loss_train, loss_val,metric_train, metric_val, recall_score ,i=i, calc_recall=calc_recall)\n",
        "\n",
        "        if checkpoint:\n",
        "            #path = f'checkpoints/epoch{i}_recanet_dunnhumby_20.pth'\n",
        "            #torch.save(model.state_dict(), path)\n",
        "            np.save(f'/content/drive/MyDrive/ckpt_instacart/loss_train.npy', loss_train)\n",
        "            np.save(f'/content/drive/MyDrive/ckpt_instacart/loss_val.npy', loss_val)\n",
        "            np.save(f'/content/drive/MyDrive/ckpt_instacart/metric_train.npy', metric_train)\n",
        "            np.save(f'/content/drive/MyDrive/ckpt_instacart/metric_val.npy', metric_val)\n",
        "            #np.save('checkpoints/recall_score.npy', recall_score)\n",
        "        \n",
        "    if calc_recall:\n",
        "        #print(recall_score)\n",
        "        print('Best epoch:', np.argmax(np.array(recall_score))) \n",
        "    return best_model, loss_train, loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQeRUV_n0gS9"
      },
      "outputs": [],
      "source": [
        "def predict(model, test_loader):\n",
        "    \n",
        "    test_items, test_users, test_history2, test_labels = dataset.create_test_data('test')\n",
        "    preds = []\n",
        "    pred_scores = []\n",
        "    \n",
        "    for x1, x2,  x4, y in tqdm(test_loader):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x1, x2, x4)\n",
        "            pred = [round(value) for value in y_pred.flatten().tolist()]\n",
        "            preds.extend(pred)\n",
        "            pred_scores.extend(y_pred.flatten().tolist())\n",
        "            \n",
        "   \n",
        "    prediction_baskets = {}\n",
        "    prediction_scores = {}\n",
        "    for user in dataset.test_users:\n",
        "        top_items = []\n",
        "        if user in dataset.user_id_mapper:\n",
        "            user_id = dataset.user_id_mapper[user]\n",
        "            indices = np.argwhere(test_users == user_id)\n",
        "            item_scores = np.array(pred_scores)[indices].flatten()\n",
        "            item_ids = test_items[indices].flatten()\n",
        "            item_score_dic = {}\n",
        "            for i, item_id in enumerate(item_ids):\n",
        "                item_score_dic[dataset.id_item_mapper[item_id]] = item_scores[i]\n",
        "            sorted_item_scores = sorted(item_score_dic.items(), key= lambda x: x[1], reverse = True)\n",
        "            top_items = [x[0] for x in sorted_item_scores]\n",
        "            prediction_scores[user] = sorted_item_scores\n",
        "            \n",
        "        prediction_baskets[user] = top_items\n",
        "\n",
        "    return prediction_baskets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwPIsu1S0gS-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def recall_k(y_true, y_pred, k ):\n",
        "    a = len(set(y_pred[:k]).intersection(set(y_true)))\n",
        "    b = len(set(y_true))\n",
        "    return a/b\n",
        "\n",
        "\n",
        "def ndcg_k(y_true, y_pred, k):\n",
        "    a = 0\n",
        "    for i,x in enumerate(y_pred[:k]):\n",
        "        if x in y_true:\n",
        "            a+= 1/np.log2(i+2)\n",
        "    b = 0\n",
        "    for i in range(k):#range(min(k,len(set(y_true)))):\n",
        "        b +=1/np.log2(i+2)\n",
        "    return a/b\n",
        "\n",
        "\n",
        "def repeat_score_item(train_baskets):\n",
        "    user_item = train_baskets.groupby(['user_id','item_id']).size().to_frame(name = 'item_count'). \\\n",
        "        reset_index()\n",
        "\n",
        "    user_train_baskets_df = train_baskets[['user_id','basket_id']].drop_duplicates().groupby(['user_id'])\\\n",
        "        .size().reset_index()\n",
        "    user_train_baskets_dict = dict(zip( user_train_baskets_df['user_id'],user_train_baskets_df[0]))\n",
        "\n",
        "\n",
        "    user_baskets = train_baskets[['user_id','date','basket_id']].drop_duplicates(). \\\n",
        "        sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()\n",
        "    user_baskets_dict = dict(zip(user_baskets['user_id'],user_baskets['basket_id']))\n",
        "\n",
        "    basket_items = train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()\n",
        "    basket_items_dict = dict(zip(basket_items['basket_id'],basket_items['item_id']))\n",
        "\n",
        "    rep_score = {}\n",
        "    for user in user_baskets_dict:\n",
        "        baskets = user_baskets_dict[user]\n",
        "        rep_score[user] = 0\n",
        "        for i in range(len(baskets)):\n",
        "            next_basket = baskets[i]\n",
        "            history_baskets = baskets[:i]\n",
        "\n",
        "            history_items = []\n",
        "            for basket in history_baskets:\n",
        "                for item in basket_items_dict[basket]:\n",
        "                    history_items.append(item)\n",
        "            history_items = set(history_items)\n",
        "            score = 0\n",
        "            for item in basket_items_dict[next_basket]:\n",
        "                if item in history_items:\n",
        "                    score +=1\n",
        "            rep_score[user] += score/len(basket_items_dict[next_basket])\n",
        "        rep_score[user]/=len(baskets)\n",
        "    return rep_score\n",
        "\n",
        "def repeat_score_user(train_baskets):\n",
        "    user_baskets = train_baskets[['user_id','date','basket_id']].drop_duplicates().\\\n",
        "        sort_values(['user_id','date'],ascending=True).groupby(['user_id'])['basket_id'].apply(list).reset_index()\n",
        "    user_baskets_dict = dict(zip(user_baskets['user_id'],user_baskets['basket_id']))\n",
        "\n",
        "    basket_items = train_baskets.groupby(['basket_id'])['item_id'].apply(list).reset_index()\n",
        "    basket_items_dict = dict(zip(basket_items['basket_id'],basket_items['item_id']))\n",
        "\n",
        "    rep_score = {}\n",
        "    for user in user_baskets_dict:\n",
        "        baskets = user_baskets_dict[user]\n",
        "        rep_score[user] = 0\n",
        "        for i in range(len(baskets)):\n",
        "            next_basket = baskets[i]\n",
        "            history_baskets = baskets[:i]\n",
        "\n",
        "            history_items = []\n",
        "            for basket in history_baskets:\n",
        "                for item in basket_items_dict[basket]:\n",
        "                    history_items.append(item)\n",
        "            history_items = set(history_items)\n",
        "            score = 0\n",
        "            for item in basket_items_dict[next_basket]:\n",
        "                if item in history_items:\n",
        "                    score +=1\n",
        "            rep_score[user] += score/len(basket_items_dict[next_basket])\n",
        "        rep_score[user]/=len(baskets)\n",
        "    return rep_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2UrjZf2Y0gS-"
      },
      "outputs": [],
      "source": [
        "info = train(model_new, epochs=2, calc_recall=False, checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1_Dgki4Yr6w",
        "outputId": "641c577b-0c94-41f7-ff29-081cb8f6a86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQB83T6N0gS-"
      },
      "outputs": [],
      "source": [
        "#info = train(model_new, epochs=2, calc_recall=True, checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXkUNjb80gS_"
      },
      "outputs": [],
      "source": [
        "#'/kaggle/input/tr-weights/epoch_recanet_best_model_dunnhumby.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkgCbHF10gTA"
      },
      "outputs": [],
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model_new = ReCaNet(num_items=dataset.num_items, item_embed_size=item_embed_size, num_users=dataset.num_users, \n",
        "#                 user_embed_size=user_embed_size, history_len = history_len, h1 = h1,h2 = h2, h3 = h3,\n",
        "#                 h4 = h4, h5 = h5).to(device)\n",
        "\n",
        "# path_best = '/kaggle/input/tr-weights/epoch_recanet_best_model_dunnhumby.pth'\n",
        "# #'/kaggle/input/lstm-weights/epoch_recanet_best_model_dunnhumby-lstm.pth'\n",
        "# model_new.load_state_dict(torch.load(path_best))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsWKIZbn0gTA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ8U-s9KgvFx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2selV7xP0gTA"
      },
      "outputs": [],
      "source": [
        "#best_model = model_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6puEnwg0gTA"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "# test_baskets = pd.read_csv(path_test) \n",
        "# #path_test\n",
        "# #pd.read_csv(path_test)\n",
        "# user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
        "# user_test_baskets_dict = dict(zip( user_test_baskets_df['user_id'],user_test_baskets_df['item_id']))\n",
        "\n",
        "# user_predictions = predict(best_model, test_loader)\n",
        "# final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))\n",
        "# print('predictions ready',len(user_predictions))\n",
        "# print('number of final test users:',len(final_users))\n",
        "# for k in [5,10,20,'B']:\n",
        "#     print(k)\n",
        "#     recall_scores = {}\n",
        "#     ndcg_scores = {}\n",
        "#     #zero = 0\n",
        "#     for user in final_users:\n",
        "\n",
        "#         top_items = []\n",
        "#         if user in user_predictions:\n",
        "#             top_items = user_predictions[user]\n",
        "#         else:\n",
        "#             zero+=1\n",
        "\n",
        "#         if k == 'B':\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#         else:\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,k)\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,k)\n",
        "#     #print(zero)\n",
        "#     print('recall:',np.mean(list(recall_scores.values())))\n",
        "#     print('ndcg:',np.mean(list(ndcg_scores.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nrdE6Hi0gTA"
      },
      "outputs": [],
      "source": [
        "# test_baskets = pd.read_csv(path_test) \n",
        "# #path_test\n",
        "# #pd.read_csv(path_test)\n",
        "# user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
        "# user_test_baskets_dict = dict(zip( user_test_baskets_df['user_id'],user_test_baskets_df['item_id']))\n",
        "\n",
        "# user_predictions = predict(best_model, test_loader)\n",
        "# final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))\n",
        "# print('predictions ready',len(user_predictions))\n",
        "# print('number of final test users:',len(final_users))\n",
        "# for k in [5,10,20,'B']:\n",
        "#     print(k)\n",
        "#     recall_scores = {}\n",
        "#     ndcg_scores = {}\n",
        "#     #zero = 0\n",
        "#     for user in final_users:\n",
        "\n",
        "#         top_items = []\n",
        "#         if user in user_predictions:\n",
        "#             top_items = user_predictions[user]\n",
        "#         else:\n",
        "#             zero+=1\n",
        "\n",
        "#         if k == 'B':\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#         else:\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,k)\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,k)\n",
        "#     #print(zero)\n",
        "#     print('recall:',np.mean(list(recall_scores.values())))\n",
        "#     print('ndcg:',np.mean(list(ndcg_scores.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNlIAp3A0gTA"
      },
      "outputs": [],
      "source": [
        "# test_baskets = pd.read_csv(path_test) \n",
        "# #path_test\n",
        "# #pd.read_csv(path_test)\n",
        "# user_test_baskets_df = test_baskets.groupby('user_id')['item_id'].apply(list).reset_index()\n",
        "# user_test_baskets_dict = dict(zip( user_test_baskets_df['user_id'],user_test_baskets_df['item_id']))\n",
        "\n",
        "# user_predictions = predict(best_model, test_loader)\n",
        "# final_users = set(dataset.test_users).intersection(set(list(user_test_baskets_dict.keys())))\n",
        "# print('predictions ready',len(user_predictions))\n",
        "# print('number of final test users:',len(final_users))\n",
        "# for k in [5,10,20,'B']:\n",
        "#     print(k)\n",
        "#     recall_scores = {}\n",
        "#     ndcg_scores = {}\n",
        "#     #zero = 0\n",
        "#     for user in final_users:\n",
        "\n",
        "#         top_items = []\n",
        "#         if user in user_predictions:\n",
        "#             top_items = user_predictions[user]\n",
        "#         else:\n",
        "#             zero+=1\n",
        "\n",
        "#         if k == 'B':\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,len(user_test_baskets_dict[user]))\n",
        "#         else:\n",
        "#             recall_scores[user] = recall_k(user_test_baskets_dict[user],top_items,k)\n",
        "#             ndcg_scores[user] = ndcg_k(user_test_baskets_dict[user],top_items,k)\n",
        "#     #print(zero)\n",
        "#     print('recall:',np.mean(list(recall_scores.values())))\n",
        "#     print('ndcg:',np.mean(list(ndcg_scores.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ojY74z7SdJq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzWulXLMSdGY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "333057e4c4de41a1851269c7c46768cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8151a5ff0c774445a3e869417ae1c46a",
            "placeholder": "​",
            "style": "IPY_MODEL_c722d319f9f24b569dc75c2456f56382",
            "value": " 0/8167 [00:07&lt;?, ?it/s]"
          }
        },
        "7302dfd81d334fa38f727853efb10ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752b52a1a5e24cd4beefc4734c9d93dd",
            "max": 8167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7389940e5f84c34bd11978e94448b91",
            "value": 0
          }
        },
        "752b52a1a5e24cd4beefc4734c9d93dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8151a5ff0c774445a3e869417ae1c46a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baced45ab7af4f27b76cadef2d9b0cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0ccaff87cab491698d8c9ab95037fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65594118fa64f09bb9cfe9d86c32d97",
            "placeholder": "​",
            "style": "IPY_MODEL_cf78050cb973474ab704684da63f9549",
            "value": "  0%"
          }
        },
        "c722d319f9f24b569dc75c2456f56382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf78050cb973474ab704684da63f9549": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d65594118fa64f09bb9cfe9d86c32d97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7389940e5f84c34bd11978e94448b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f504db3697184171ba2461e502ce6d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0ccaff87cab491698d8c9ab95037fe2",
              "IPY_MODEL_7302dfd81d334fa38f727853efb10ba5",
              "IPY_MODEL_333057e4c4de41a1851269c7c46768cb"
            ],
            "layout": "IPY_MODEL_baced45ab7af4f27b76cadef2d9b0cc3"
          }
        },
        "38c8939aa01b467b8e966c3fbad6c89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1ab8d7c2425404db947bf004497d39f",
              "IPY_MODEL_7aaa3fc7cd1842e3983017906fe58447",
              "IPY_MODEL_0a28ee35fd4e406f81f0e16ec0163674"
            ],
            "layout": "IPY_MODEL_085ae608526545ee9659467431ef45b1"
          }
        },
        "a1ab8d7c2425404db947bf004497d39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca1f9596a6f4fd9a0f7955699da9ba2",
            "placeholder": "​",
            "style": "IPY_MODEL_725325da0f7943f7b8ac9b4a9227ca37",
            "value": "100%"
          }
        },
        "7aaa3fc7cd1842e3983017906fe58447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac7b075a5fe24518ac1d9771b98c729d",
            "max": 2483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e0564aabfb94d8d9a44f188d0ac17eb",
            "value": 2483
          }
        },
        "0a28ee35fd4e406f81f0e16ec0163674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584e71fb7a8a43498e21d8f84b4b309c",
            "placeholder": "​",
            "style": "IPY_MODEL_bb648c7e29764eb491164490e2258741",
            "value": " 2483/2483 [00:06&lt;00:00, 392.83it/s]"
          }
        },
        "085ae608526545ee9659467431ef45b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca1f9596a6f4fd9a0f7955699da9ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725325da0f7943f7b8ac9b4a9227ca37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac7b075a5fe24518ac1d9771b98c729d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0564aabfb94d8d9a44f188d0ac17eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "584e71fb7a8a43498e21d8f84b4b309c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb648c7e29764eb491164490e2258741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}